{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Analysis NYC TLC Trips Records Data Feb 2021\n",
    "---\n",
    "<sub>Muhammad Difagama Ivanka</sub>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(233)\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('nyc_spark') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 20.7M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  1 20.7M    1  326k    0     0   140k      0  0:02:31  0:00:02  0:02:29  140k\n",
      " 12 20.7M   12 2737k    0     0   841k      0  0:00:25  0:00:03  0:00:22  842k\n",
      " 17 20.7M   17 3655k    0     0   859k      0  0:00:24  0:00:04  0:00:20  859k\n",
      " 17 20.7M   17 3767k    0     0   704k      0  0:00:30  0:00:05  0:00:25  760k\n",
      " 19 20.7M   19 4068k    0     0   650k      0  0:00:32  0:00:06  0:00:26  829k\n",
      " 20 20.7M   20 4416k    0     0   605k      0  0:00:35  0:00:07  0:00:28  821k\n",
      " 22 20.7M   22 4832k    0     0   579k      0  0:00:36  0:00:08  0:00:28  411k\n",
      " 25 20.7M   25 5326k    0     0   575k      0  0:00:36  0:00:09  0:00:27  334k\n",
      " 27 20.7M   27 5763k    0     0   562k      0  0:00:37  0:00:10  0:00:27  406k\n",
      " 38 20.7M   38 8134k    0     0   722k      0  0:00:29  0:00:11  0:00:18  812k\n",
      " 45 20.7M   45 9724k    0     0   777k      0  0:00:27  0:00:12  0:00:15 1019k\n",
      " 61 20.7M   61 12.8M    0     0   993k      0  0:00:21  0:00:13  0:00:08 1696k\n",
      " 73 20.7M   73 15.3M    0     0  1101k      0  0:00:19  0:00:14  0:00:05 2075k\n",
      " 85 20.7M   85 17.8M    0     0  1195k      0  0:00:17  0:00:15  0:00:02 2494k\n",
      " 97 20.7M   97 20.2M    0     0  1275k      0  0:00:16  0:00:16 --:--:-- 2519k\n",
      "100 20.7M  100 20.7M    0     0  1292k      0  0:00:16  0:00:16 --:--:-- 2925k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 15 1118k   15  174k    0     0   112k      0  0:00:09  0:00:01  0:00:08  112k\n",
      "100 1118k  100 1118k    0     0   490k      0  0:00:02  0:00:02 --:--:--  492k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 10.1M    0 16384    0     0  14415      0  0:12:18  0:00:01  0:12:17 14422\n",
      " 10 10.1M   10 1070k    0     0   502k      0  0:00:20  0:00:02  0:00:18  502k\n",
      " 34 10.1M   34 3570k    0     0   895k      0  0:00:11  0:00:03  0:00:08  896k\n",
      " 34 10.1M   34 3586k    0     0   847k      0  0:00:12  0:00:04  0:00:08  847k\n",
      " 35 10.1M   35 3666k    0     0   695k      0  0:00:14  0:00:05  0:00:09  725k\n",
      " 42 10.1M   42 4380k    0     0   709k      0  0:00:14  0:00:06  0:00:08  866k\n",
      " 63 10.1M   63 6567k    0     0   920k      0  0:00:11  0:00:07  0:00:04 1099k\n",
      " 66 10.1M   66 6946k    0     0   832k      0  0:00:12  0:00:08  0:00:04  773k\n",
      " 67 10.1M   67 7026k    0     0   765k      0  0:00:13  0:00:09  0:00:04  695k\n",
      " 69 10.1M   69 7249k    0     0   708k      0  0:00:14  0:00:10  0:00:04  720k\n",
      " 71 10.1M   71 7409k    0     0   656k      0  0:00:15  0:00:11  0:00:04  592k\n",
      " 73 10.1M   73 7678k    0     0   632k      0  0:00:16  0:00:12  0:00:04  222k\n",
      " 78 10.1M   78 8165k    0     0   620k      0  0:00:16  0:00:13  0:00:03  253k\n",
      " 84 10.1M   84 8790k    0     0   621k      0  0:00:16  0:00:14  0:00:02  355k\n",
      "100 10.1M  100 10.1M    0     0   703k      0  0:00:14  0:00:14 --:--:--  692k\n"
     ]
    }
   ],
   "source": [
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet > yellow_tripdata_2021-02.parquet\n",
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-02.parquet > green_tripdata_2021-02.parquet\n",
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-02.parquet > fhv_tripdata_2021-02.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('yellow_tripdata_2021-02.parquet')\n",
    "df_green = spark.read.parquet('green_tripdata_2021-02.parquet')\n",
    "df_fhv = spark.read.parquet('fhv_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(the_df, pickup_col: str, dropoff_col: str):\n",
    "    pickup_col = F.col(pickup_col)\n",
    "    dropoff_col = F.col(dropoff_col)\n",
    "\n",
    "    trips_cnt = the_df.where((pickup_col >= \"2021-02-01 00:00:00\")\n",
    "    & (dropoff_col< \"2021-02-29 00:00:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[VendorID: bigint, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: double, trip_distance: double, RatecodeID: double, store_and_fwd_flag: string, PULocationID: bigint, DOLocationID: bigint, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, airport_fee: double] \n",
      "\n",
      "DataFrame[VendorID: bigint, lpep_pickup_datetime: timestamp, lpep_dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: double, PULocationID: bigint, DOLocationID: bigint, passenger_count: double, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, ehail_fee: int, improvement_surcharge: double, total_amount: double, payment_type: double, trip_type: double, congestion_surcharge: double] \n",
      "\n",
      "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropOff_datetime: timestamp, PUlocationID: double, DOlocationID: double, SR_Flag: int, Affiliated_base_number: string]\n"
     ]
    }
   ],
   "source": [
    "print(df_yellow, \"\\n\")\n",
    "print(df_green, \"\\n\")\n",
    "print(df_fhv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How many taxi trips were there on February 15?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Taxi Trips on 15 February 2021\t\t\t: 43734\n",
      "Green Taxi Trips on 15 February 2021\t\t\t: 1798\n",
      "For-Hire Vehicle (FHV) Taxi Trips on 15 February 2021\t: 35523\n",
      "All Taxi Total Trips on 15 February 2021\t\t: 81055\n"
     ]
    }
   ],
   "source": [
    "def total_trips_cnt(the_df, pickup_time_col):\n",
    "    trips_cnt = the_df.where((the_df[pickup_time_col] >= \"2021-02-15 00:00:00\")\n",
    "    & (the_df[pickup_time_col] < \"2021-02-16 00:00:00\")).count()\n",
    "    return trips_cnt\n",
    "\n",
    "yel_cnt = total_trips_cnt(df_yellow, 'tpep_pickup_datetime')\n",
    "grn_cnt = total_trips_cnt(df_green, 'lpep_pickup_datetime')\n",
    "fhv_cnt = total_trips_cnt(df_fhv, 'pickup_datetime')\n",
    "\n",
    "print(f\"Yellow Taxi Trips on 15 February 2021\\t\\t\\t: {yel_cnt}\")\n",
    "print(f\"Green Taxi Trips on 15 February 2021\\t\\t\\t: {grn_cnt}\")\n",
    "print(f\"For-Hire Vehicle (FHV) Taxi Trips on 15 February 2021\\t: {fhv_cnt}\")\n",
    "print(f\"All Taxi Total Trips on 15 February 2021\\t\\t: {np.sum([yel_cnt,grn_cnt,fhv_cnt])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The longest trip for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------+\n",
      "|Pickup Date|Longest Duration (minutes)|\n",
      "+-----------+--------------------------+\n",
      "| 2009-01-01|         573.4166666666666|\n",
      "| 2021-02-15|        1439.0166666666667|\n",
      "| 2021-02-02|        1438.6666666666667|\n",
      "| 2021-02-26|        1438.7333333333333|\n",
      "| 2021-02-21|        1439.0666666666666|\n",
      "| 2021-02-05|        1439.5833333333333|\n",
      "| 2021-02-10|        1439.2333333333333|\n",
      "| 2021-02-01|                    1421.8|\n",
      "| 2021-02-06|                    1439.0|\n",
      "| 2009-01-02|                      7.35|\n",
      "| 2021-02-19|        1438.6333333333334|\n",
      "| 2021-02-20|                   1439.05|\n",
      "| 2021-02-08|                   1439.45|\n",
      "| 2021-02-09|        1438.8333333333333|\n",
      "| 2021-02-11|        1439.0833333333333|\n",
      "| 2021-02-17|        1439.6666666666667|\n",
      "| 2021-02-25|        1439.7833333333333|\n",
      "| 2021-03-01|                    1438.8|\n",
      "| 2021-02-27|        1438.8333333333333|\n",
      "| 2021-02-24|        1439.5166666666667|\n",
      "+-----------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Duration (minutes)\n",
    "def duration_trips_cal(the_df, pickup_col: str, dropoff_col: str, res_name_col = 'Longest Duration (minutes)'):\n",
    "    '''in minute(s)'''\n",
    "    duration_col = 'trip_duration'\n",
    "    date_col = F.to_date(pickup_col)\n",
    "\n",
    "    df_temp = the_df.withColumn(\n",
    "        duration_col,\n",
    "        (F.col(dropoff_col).cast('long') - F.col(pickup_col).cast('long'))/60\n",
    "    )\n",
    "    df_temp = df_temp.groupBy(date_col).max(duration_col)\n",
    "    df_temp = df_temp.withColumnRenamed(f'max({duration_col})', res_name_col)\\\n",
    "        .withColumnRenamed(f'to_date({pickup_col})', 'Pickup Date')\n",
    "    df_temp = df_temp.orderBy(date_col).asc()\n",
    "    return df_temp\n",
    "\n",
    "duration_trips_cal(df_yellow, 'tpep_pickup_datetime', 'tpep_dropoff_datetime').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: bigint, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: double, trip_distance: double, RatecodeID: double, store_and_fwd_flag: string, PULocationID: bigint, DOLocationID: bigint, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, airport_fee: double]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yellow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top 5 Most frequent `dispatching_base_num`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top 5 Most common location pairs (PUlocationID and DOlocationID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "494c90ffea2053e1d478862651e70463b529ab2d579abda18db8e48081cff208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
